{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nabil/ml_contests/cooking_activity/cook2020_tutorials\n",
      "/home/nabil/ml_contests/cooking_activity/cook2020_tutorials/train/train\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from tsfresh import extract_features, extract_relevant_features\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "cdir = os.getcwd()\n",
    "print(cdir)\n",
    "os.chdir(cdir + '/train/train')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head CNN–RNN for multi-time series anomaly detection: An industrial case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sensordata, timestamps):\n",
    "    # process\n",
    "    \n",
    "    return sensordata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2', 'X3', 'Y3', 'Z3', 'X4', 'Y4', 'Z4', 'X5', 'Y5', 'Z5', 'X6', 'Y6', 'Z6', 'X7', 'Y7', 'Z7', 'X8', 'Y8', 'Z8', 'X9', 'Y9', 'Z9', 'X10', 'Y10', 'Z10', 'X11', 'Y11', 'Z11', 'X12', 'Y12', 'Z12', 'X13', 'Y13', 'Z13', 'X14', 'Y14', 'Z14', 'X15', 'Y15', 'Z15', 'X16', 'Y16', 'Z16', 'X17', 'Y17', 'Z17', 'X18', 'Y18', 'Z18', 'X19', 'Y19', 'Z19', 'X20', 'Y20', 'Z20', 'X21', 'Y21', 'Z21', 'X22', 'Y22', 'Z22', 'X23', 'Y23', 'Z23', 'X24', 'Y24', 'Z24', 'X25', 'Y25', 'Z25', 'X26', 'Y26', 'Z26', 'X27', 'Y27', 'Z27', 'X28', 'Y28', 'Z28', 'X29', 'Y29', 'Z29']\n"
     ]
    }
   ],
   "source": [
    "mocap_axis = []\n",
    "for xyz in range(1,30):\n",
    "    mocap_axis.append('X'+str(xyz))\n",
    "    mocap_axis.append('Y'+str(xyz))\n",
    "    mocap_axis.append('Z'+str(xyz))\n",
    "print(mocap_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d8f8930489f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "labels.iloc[125]['micro'].split(',')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['right_arm/*.csv', 'right_wrist/*.csv', 'left_hip/*.csv', 'left_wrist/*.csv']\n",
      "                                           0\n",
      "0           subject2_file_457,sandwich,Take,\n",
      "1      subject2_file_679,sandwich,Wash,Take,\n",
      "2        subject2_file_95,sandwich,Cut,Wash,\n",
      "3  subject2_file_899,sandwich,other,Cut,Put,\n",
      "4            subject2_file_368,sandwich,Put,\n",
      "                             file_id     macro           micro\n",
      "file_id                                                       \n",
      "subject2_file_457  subject2_file_457  sandwich           Take,\n",
      "subject2_file_679  subject2_file_679  sandwich      Wash,Take,\n",
      "subject2_file_95    subject2_file_95  sandwich       Cut,Wash,\n",
      "subject2_file_899  subject2_file_899  sandwich  other,Cut,Put,\n",
      "subject2_file_368  subject2_file_368  sandwich            Put,\n"
     ]
    }
   ],
   "source": [
    "# there are 3 subjects\n",
    "subject1 = {}\n",
    "subject2 = {}\n",
    "subject3 = {}\n",
    "\n",
    "# we will load every single right arm data, separate based on subject id, do feature extraction, run t-SNE\n",
    "\n",
    "all_sensors = ['right_arm', 'right_wrist', 'left_hip', 'left_wrist'] #, 'mocap']\n",
    "\n",
    "data_folder = [f'{sensor}/*.csv' for sensor in all_sensors]\n",
    "\n",
    "print(data_folder)\n",
    "\n",
    "files = glob.glob(data_folder[0])\n",
    "\n",
    "# processing will be done as numpy array\n",
    "\n",
    "subject1['data'] = []\n",
    "subject2['data'] = []\n",
    "subject3['data'] = []\n",
    " \n",
    "# data format : data -> id, process(timeseries, timestamps) : right_arm as numpy array (missing data will be imputed)\n",
    "\n",
    "\n",
    "# id is generated by concatenating INT(subject+trial)\n",
    "\n",
    "for f in files:\n",
    "    #print(f)\n",
    "    c_sub = {} # current subject\n",
    "    if f.split('/')[1].split('_')[0] == 'subject1':\n",
    "        \n",
    "        c_sub['id'] = int(f.split('/')[1].split('_')[0][-1] + f.split('/')[1].split('_')[2].split('.')[0])\n",
    "        #print(ra['id'])\n",
    "        \n",
    "        for sensor in all_sensors:\n",
    "            #print(sensor)\n",
    "            ra = pd.read_csv(sensor + '/' + f.split('/')[1])\n",
    "            #print(ra)\n",
    "            ra.sort_values(by=['timestamp'], inplace = True)\n",
    "            c_sub[sensor] = {}\n",
    "            if sensor == 'mocap':\n",
    "                for axis in mocap_axis:\n",
    "                    c_sub[sensor][axis] = process(ra[axis], ra['timestamp'])\n",
    "            else:\n",
    "                for axis in ['X', 'Y', 'Z']:\n",
    "                    c_sub[sensor][axis] = process(ra[axis], ra['timestamp'])\n",
    "                \n",
    "        subject1['data'].append(c_sub)\n",
    "\n",
    "            \n",
    "    elif f.split('/')[1].split('_')[0] == 'subject2':\n",
    "        \n",
    "        c_sub['id'] = int(f.split('/')[1].split('_')[0][-1] + f.split('/')[1].split('_')[2].split('.')[0])\n",
    "        #print(ra['id'])\n",
    "        \n",
    "        for sensor in all_sensors:\n",
    "            ra = pd.read_csv(sensor + '/' + f.split('/')[1])\n",
    "            #print(ra)\n",
    "            ra.sort_values(by=['timestamp'], inplace = True)\n",
    "            c_sub[sensor] = {}\n",
    "            if sensor == 'mocap':\n",
    "                for axis in mocap_axis:\n",
    "                    c_sub[sensor][axis] = process(ra[axis], ra['timestamp'])\n",
    "            else:\n",
    "                for axis in ['X', 'Y', 'Z']:\n",
    "                    c_sub[sensor][axis] = process(ra[axis], ra['timestamp'])\n",
    "                \n",
    "        subject2['data'].append(c_sub)\n",
    "            \n",
    "    elif f.split('/')[1].split('_')[0] == 'subject3':\n",
    "        \n",
    "        c_sub['id'] = int(f.split('/')[1].split('_')[0][-1] + f.split('/')[1].split('_')[2].split('.')[0])\n",
    "        #print(ra['id'])\n",
    "        \n",
    "        for sensor in all_sensors:\n",
    "            ra = pd.read_csv(sensor + '/' + f.split('/')[1])\n",
    "            #print(ra)\n",
    "            ra.sort_values(by=['timestamp'], inplace = True)\n",
    "            c_sub[sensor] = {}\n",
    "            if sensor == 'mocap':\n",
    "                for axis in mocap_axis:\n",
    "                    try:\n",
    "                        c_sub[sensor][axis] = process(ra[axis], ra['timestamp'])\n",
    "                    except:\n",
    "                        c_sub[sensor][axis] = [0]\n",
    "            else:\n",
    "                for axis in ['X', 'Y', 'Z']:\n",
    "                    c_sub[sensor][axis] = process(ra[axis], ra['timestamp'])\n",
    "        \n",
    "        subject3['data'].append(c_sub)\n",
    "    else:\n",
    "        print('either new subject or a bug')\n",
    "        \n",
    "        \n",
    "labels = pd.read_csv(\"labels.txt\", sep=' ', header=None)\n",
    "print(labels.head())\n",
    "labels = labels[0].str.split(\",\", n=2, expand=True)\n",
    "labels.columns = ['file_id', 'macro', 'micro'] #give names to the columns\n",
    "labels.index = labels['file_id'] #use the file id as index to make it searchable by file_id\n",
    "print(labels.head())\n",
    "\n",
    "# label generation\n",
    "subject1['label_mac'] = {}\n",
    "subject2['label_mac'] = {}\n",
    "subject3['label_mac'] = {}\n",
    "\n",
    "subject1['label_mic'] = {}\n",
    "subject2['label_mic'] = {}\n",
    "subject3['label_mic'] = {}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    #print(labels.iloc[i]['file_id'])\n",
    "    #print(labels.iloc[i]['file_id'].split('_')[0][-1])\n",
    "    tid = int(labels.iloc[i]['file_id'].split('_')[0][-1] + labels.iloc[i]['file_id'].split('_')[-1])\n",
    "    #print(tid)\n",
    "    label = labels.iloc[i]['macro']\n",
    "    label_mic = labels.iloc[i]['micro'].split(',')[:-1]\n",
    "    #print(label)\n",
    "    if labels.iloc[i]['file_id'].split('_')[0][-1] == '1':\n",
    "        subject1['label_mac'][tid] = label\n",
    "        subject1['label_mic'][tid] = label_mic\n",
    "    elif labels.iloc[i]['file_id'].split('_')[0][-1] == '2':\n",
    "        subject2['label_mac'][tid] = label\n",
    "        subject2['label_mic'][tid] = label_mic\n",
    "    elif labels.iloc[i]['file_id'].split('_')[0][-1] == '3':\n",
    "        subject3['label_mac'][tid] = label\n",
    "        subject3['label_mic'][tid] = label_mic\n",
    "    else:\n",
    "        print('some bug')\n",
    "        \n",
    "        \n",
    "# re-formatting dataset for training\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "y_ml = []  # multi-label\n",
    "\n",
    "for i in range(len(subject1['data'])):\n",
    "    tid = subject1['data'][i]['id']\n",
    "    y.append(subject1['label_mac'][tid])\n",
    "    y_ml.append(subject1['label_mic'][tid])\n",
    "    # X shape -> [ip1, ip2, ip3, ip4] ip1 = (80, len, channel) -> 4, 80, len\n",
    "\n",
    "for sensor in all_sensors:\n",
    "    cs_data = []\n",
    "    for i in range(len(subject1['data'])):\n",
    "        sub_data = []\n",
    "        if sensor == 'mocap':\n",
    "            for sig in mocap_axis:\n",
    "                sub_data.append(np.array(subject1['data'][i][sensor][sig]))\n",
    "        else:\n",
    "            for sig in ['X', 'Y', 'Z']:\n",
    "                sub_data.append(np.array(subject1['data'][i][sensor][sig]))\n",
    "        sub_data = np.array(sub_data)\n",
    "        # print(sub_data.shape)\n",
    "        sub_data = np.swapaxes(sub_data, 0, 1)\n",
    "        cs_data.append(sub_data)\n",
    "    X.append(cs_data)\n",
    "    \n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "vec = label_encoder.fit_transform(y)\n",
    "\n",
    "y_ohe = to_categorical(vec,len(set(vec)))\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_ml_ohe = mlb.fit_transform(y_ml)\n",
    "\n",
    "# validatin on subject2, 3\n",
    "\n",
    "# re-formatting dataset for training\n",
    "\n",
    "X2 = []\n",
    "y2 = []\n",
    "y_ml2 = []\n",
    "\n",
    "for i in range(len(subject2['data'])):\n",
    "    tid = subject2['data'][i]['id']\n",
    "    y2.append(subject2['label_mac'][tid])\n",
    "    y_ml2.append(subject2['label_mic'][tid])\n",
    "    # X shape -> [ip1, ip2, ip3, ip4] ip1 = (80, len, channel) -> 4, 80, len\n",
    "\n",
    "for sensor in all_sensors:\n",
    "    cs_data = []\n",
    "    for i in range(len(subject2['data'])):\n",
    "        sub_data = []\n",
    "        if sensor == 'mocap':\n",
    "            for sig in mocap_axis:\n",
    "                sub_data.append(np.array(subject2['data'][i][sensor][sig]))\n",
    "        else:\n",
    "            for sig in ['X', 'Y', 'Z']:\n",
    "                sub_data.append(np.array(subject2['data'][i][sensor][sig]))\n",
    "        sub_data = np.array(sub_data)\n",
    "        # print(sub_data.shape)\n",
    "        sub_data = np.swapaxes(sub_data, 0, 1)\n",
    "        cs_data.append(sub_data)\n",
    "    X2.append(cs_data)\n",
    "    \n",
    "label_encoder = LabelEncoder()\n",
    "vec = label_encoder.fit_transform(y2)\n",
    "\n",
    "y2_ohe = to_categorical(vec,len(set(vec)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_ml2_ohe = mlb.fit_transform(y_ml2)\n",
    "\n",
    "# validatin on subject2, 3\n",
    "\n",
    "# re-formatting dataset for training\n",
    "\n",
    "X3 = []\n",
    "y3 = []\n",
    "y_ml3 = []\n",
    "\n",
    "for i in range(len(subject3['data'])):\n",
    "    tid = subject3['data'][i]['id']\n",
    "    y3.append(subject3['label_mac'][tid])\n",
    "    y_ml3.append(subject3['label_mic'][tid])\n",
    "    # X shape -> [ip1, ip2, ip3, ip4] ip1 = (80, len, channel) -> 4, 80, len\n",
    "\n",
    "for sensor in all_sensors:\n",
    "    cs_data = []\n",
    "    for i in range(len(subject3['data'])):\n",
    "        sub_data = []\n",
    "        if sensor == 'mocap':\n",
    "            for sig in mocap_axis:\n",
    "                sub_data.append(np.array(subject3['data'][i][sensor][sig]))\n",
    "        else:\n",
    "            for sig in ['X', 'Y', 'Z']:\n",
    "                sub_data.append(np.array(subject3['data'][i][sensor][sig]))\n",
    "        sub_data = np.array(sub_data)\n",
    "        # print(sub_data.shape)\n",
    "        sub_data = np.swapaxes(sub_data, 0, 1)\n",
    "        cs_data.append(sub_data)\n",
    "    X3.append(cs_data)\n",
    "    \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "vec = label_encoder.fit_transform(y3)\n",
    "\n",
    "y3_ohe = to_categorical(vec,len(set(vec)))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_ml3_ohe = mlb.fit_transform(y_ml3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cereal', 'fruitsalad', 'sandwich'], dtype='<U10')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Add', 'Cut', 'Mix', 'Open', 'Peel', 'Pour', 'Put', 'Take', 'Wash',\n",
       "       'other'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subject1['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4, 80, len, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, LSTM, Bidirectional, Input, GlobalMaxPooling1D, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right_arm', 'right_wrist', 'left_hip', 'left_wrist']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-head\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    n_sensors = 4 #5\n",
    "    n_filters = [3, 3, 3, 3] # , 3*29] # X, Y, Z\n",
    "\n",
    "\n",
    "    # make the input layers\n",
    "    ips = []\n",
    "\n",
    "    n_f = 0\n",
    "    for sensor in all_sensors:\n",
    "        ips.append(Input(shape = (None, n_filters[n_f]), name = sensor + '_ipX'))\n",
    "        n_f += 1\n",
    "\n",
    "    print(f'Input tensors: {ips}')\n",
    "\n",
    "    convs = []\n",
    "    for ip in ips:\n",
    "        conv = Conv1D(filters = 16, kernel_size = 1, padding = 'same', activation = None, name = ip.name.split(':')[0] + '_conv')(ip)\n",
    "        convs.append(conv)\n",
    "        \n",
    "    # batch-normalization\n",
    "    bns = []\n",
    "    for conv in convs:\n",
    "        bn = BatchNormalization()(conv)\n",
    "        bns.append(bn)\n",
    "\n",
    "    convs2 = []\n",
    "    for bn in bns:\n",
    "        conv2 = Conv1D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu')(bn)\n",
    "        convs2.append(conv2)\n",
    "        \n",
    "    # batch-normalization\n",
    "    bns2 = []\n",
    "    for conv2 in convs2:\n",
    "        bn2 = BatchNormalization()(conv2)\n",
    "        bns2.append(bn2)\n",
    "\n",
    "\n",
    "\n",
    "    # concatenation\n",
    "\n",
    "    concat = Concatenate(axis=1, name = 'concat')(bns2)\n",
    "\n",
    "    concat_conv = Conv1D(filters = 16, kernel_size = 1, padding = 'same', activation = None, name = 'concat_conv')(concat)\n",
    "    \n",
    "    bn3 = BatchNormalization()(concat_conv)\n",
    "    con_conv2 = Conv1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', name = 'con_conv2')(bn3)\n",
    "    bn4 = BatchNormalization()(con_conv2)\n",
    "    gmp = GlobalMaxPooling1D()(bn4)\n",
    "\n",
    "    # simple FC\n",
    "    final_softmax = Dense(3, activation = 'softmax')(gmp)\n",
    "    simple_conv = Model(ips, final_softmax)\n",
    "    return simple_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensors: [<tf.Tensor 'right_arm_ipX:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'right_wrist_ipX:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_hip_ipX:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_wrist_ipX:0' shape=(None, None, 3) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "simple_conv = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "right_arm_ipX (InputLayer)      [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_wrist_ipX (InputLayer)    [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "left_hip_ipX (InputLayer)       [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "left_wrist_ipX (InputLayer)     [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_arm_ipX_conv (Conv1D)     (None, None, 16)     64          right_arm_ipX[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "right_wrist_ipX_conv (Conv1D)   (None, None, 16)     64          right_wrist_ipX[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "left_hip_ipX_conv (Conv1D)      (None, None, 16)     64          left_hip_ipX[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "left_wrist_ipX_conv (Conv1D)    (None, None, 16)     64          left_wrist_ipX[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 16)     64          right_arm_ipX_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 16)     64          right_wrist_ipX_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 16)     64          left_hip_ipX_conv[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 16)     64          left_wrist_ipX_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 16)     784         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 16)     784         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 16)     784         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 16)     784         batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 16)     64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 16)     64          conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 16)     64          conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 16)     64          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, None, 16)     0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concat_conv (Conv1D)            (None, None, 16)     272         concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 16)     64          concat_conv[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "con_conv2 (Conv1D)              (None, None, 32)     1568        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 32)     128         con_conv2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 32)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            99          global_max_pooling1d[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 6,035\n",
      "Trainable params: 5,683\n",
      "Non-trainable params: 352\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_conv.compile(loss='categorical_crossentropy', metrics=['acc', 'mae'], optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cereal',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'fruitsalad',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'fruitsalad',\n",
       " 'fruitsalad',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'fruitsalad',\n",
       " 'fruitsalad',\n",
       " 'cereal',\n",
       " 'fruitsalad',\n",
       " 'fruitsalad',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'fruitsalad',\n",
       " 'fruitsalad',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'fruitsalad',\n",
       " 'fruitsalad',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'fruitsalad',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'fruitsalad',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'fruitsalad',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'fruitsalad',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'cereal',\n",
       " 'fruitsalad',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'sandwich',\n",
       " 'cereal',\n",
       " 'sandwich',\n",
       " 'sandwich']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0, 0, 0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(simple_conv, X, y_ohe, EPOCH, train_split=1.0):\n",
    "    # single zero padding for NULL datas\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            if len(X) == 5 and i == len(X)-1: # assuming mocap always comes in the last index\n",
    "                if len(X[i][j]) == 0:\n",
    "                    X[i][j] = np.zeros((1, 87))\n",
    "            else:\n",
    "                if len(X[i][j]) == 0:\n",
    "                    X[i][j] = np.array([[0, 0, 0]])\n",
    "\n",
    "    report_acc = 0.0\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f'Training epoch {epoch} ...')\n",
    "        avg_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        avg_mae = 0.0\n",
    "\n",
    "        for sample_i in tqdm(range(int(len(X[0])*train_split))):\n",
    "            [train_loss, acc, mae]   =  simple_conv.train_on_batch([X[0][sample_i].reshape(1,len(X[0][sample_i]),3), \n",
    "                                        X[1][sample_i].reshape(1,len(X[1][sample_i]),3), \n",
    "                                        X[2][sample_i].reshape(1,len(X[2][sample_i]),3), \n",
    "                                        X[3][sample_i].reshape(1,len(X[3][sample_i]),3)],\n",
    "                                        #X[4][sample_i].reshape(1,len(X[4][sample_i]),3*29),], \n",
    "                                        y_ohe[sample_i].reshape(1,3))\n",
    "            avg_loss += train_loss/(len(X[0])*train_split)\n",
    "            avg_acc += acc/(len(X[0])*train_split)\n",
    "            avg_mae += mae/(len(X[0])*train_split)\n",
    "        report_acc = avg_acc\n",
    "        print(f'acc: {avg_acc} mae: {avg_mae} loss: {avg_loss}')\n",
    "        print('Running validation ...')\n",
    "        avg_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        avg_mae = 0.0\n",
    "        for sample_i in tqdm(range(int(len(X[0])*train_split), len(X[0]))):\n",
    "            [test_loss, acc, mae]   =  simple_conv.test_on_batch([X[0][sample_i].reshape(1,len(X[0][sample_i]),3), \n",
    "                                        X[1][sample_i].reshape(1,len(X[1][sample_i]),3), \n",
    "                                        X[2][sample_i].reshape(1,len(X[2][sample_i]),3), \n",
    "                                        X[3][sample_i].reshape(1,len(X[3][sample_i]),3)],\n",
    "                                        #X[4][sample_i].reshape(1,len(X[4][sample_i]),3*29)], \n",
    "                                        y_ohe[sample_i].reshape(1,3))\n",
    "            avg_loss += test_loss/(len(X[0])*(1-train_split))\n",
    "            avg_acc += acc/(len(X[0])*(1-train_split))\n",
    "            avg_mae += mae/(len(X[0])*(1-train_split))\n",
    "            \n",
    "        print(f'test acc: {avg_acc} mae: {avg_mae} loss: {avg_loss}')\n",
    "        \n",
    "    \n",
    "    return simple_conv, report_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(simple_conv, X2, y2_ohe):\n",
    "    for i in range(len(X2)):\n",
    "        for j in range(len(X2[0])):\n",
    "            if len(X2) == 5 and i == len(X2)-1: # assuming mocap always comes in the last index\n",
    "                if len(X2[i][j]) == 0:\n",
    "                    X2[i][j] = np.zeros((1, 87))\n",
    "            else:\n",
    "                if len(X2[i][j]) == 0:\n",
    "                    X2[i][j] = np.array([[0, 0, 0]])\n",
    "\n",
    "    print('Running validation on subject ...')\n",
    "    avg_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    avg_mae = 0.0\n",
    "    for sample_i in tqdm(range(int(len(X2[0])))):\n",
    "        [test_loss, acc, mae]   =  simple_conv.test_on_batch([X2[0][sample_i].reshape(1,len(X2[0][sample_i]),3), \n",
    "                                    X2[1][sample_i].reshape(1,len(X2[1][sample_i]),3), \n",
    "                                    X2[2][sample_i].reshape(1,len(X2[2][sample_i]),3), \n",
    "                                    X2[3][sample_i].reshape(1,len(X2[3][sample_i]),3)],\n",
    "                                    #X2[4][sample_i].reshape(1,len(X2[4][sample_i]),3*29)], \n",
    "                                    y2_ohe[sample_i].reshape(1,3))\n",
    "        avg_loss += test_loss/(len(X2[0]))\n",
    "        avg_acc += acc/(len(X2[0]))\n",
    "        avg_mae += mae/(len(X2[0]))\n",
    "    print(f'test acc: {avg_acc} mae: {avg_mae} loss: {avg_loss}')\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensors: [<tf.Tensor 'right_arm_ipX_1:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'right_wrist_ipX_1:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_hip_ipX_1:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_wrist_ipX_1:0' shape=(None, None, 3) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:04<01:25,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_on_batch at 0x7f33be45b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:10<00:00,  7.98it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  9%|▉         | 7/80 [00:00<00:01, 67.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3125000000000001 mae: 0.458135942854522 loss: 5.138324710551707\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:01<00:00, 65.13it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 10%|▉         | 10/105 [00:00<00:01, 86.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6249999999999998 mae: 0.2612248274297723 loss: 1.05334918890394\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 79.34it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 10%|▉         | 10/105 [00:00<00:01, 87.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.4952380952380957 mae: 0.35647896063783463 loss: 2.17277530827116\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 87.94it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7904761904761899 mae: 0.16855964953015315 loss: 0.6305255359081856\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "-------------------------------------------------\n",
      "Test phase\n",
      "Testing X1\n",
      "Running validation on subject ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:01<00:23,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:02<00:00, 27.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.46250000000000024 mae: 0.3673245084391169 loss: 2.4634018042197336\n",
      "Input tensors: [<tf.Tensor 'right_arm_ipX_2:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'right_wrist_ipX_2:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_hip_ipX_2:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_wrist_ipX_2:0' shape=(None, None, 3) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:05<01:42,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_on_batch at 0x7f33be45b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:11<00:00,  7.19it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 11%|█▏        | 9/80 [00:00<00:00, 84.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3875000000000002 mae: 0.40801310179595657 loss: 4.199722091139846\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:01<00:00, 72.22it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 10%|▉         | 10/105 [00:00<00:01, 92.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6374999999999997 mae: 0.24702048118170558 loss: 1.090476481165281\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 86.20it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  9%|▊         | 9/105 [00:00<00:01, 86.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.34285714285714297 mae: 0.4266390872287004 loss: 2.8149830681479906\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 103.26it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.733333333333333 mae: 0.22393442420522008 loss: 0.9110001192725617\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "-------------------------------------------------\n",
      "Test phase\n",
      "Testing X2\n",
      "Running validation on subject ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/105 [00:00<00:21,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/105 [00:01<00:16,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:02<00:00, 47.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.6285714285714286 mae: 0.2832621828503199 loss: 1.3326671270477155\n",
      "Input tensors: [<tf.Tensor 'right_arm_ipX_3:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'right_wrist_ipX_3:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_hip_ipX_3:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_wrist_ipX_3:0' shape=(None, None, 3) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:04<01:24,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_on_batch at 0x7f33be45b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:09<00:00,  8.26it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 12%|█▎        | 10/80 [00:00<00:00, 99.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.46250000000000024 mae: 0.36576783566480037 loss: 2.875816591159564\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:01<00:00, 78.66it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  8%|▊         | 8/105 [00:00<00:01, 72.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6874999999999996 mae: 0.2334927683153706 loss: 0.9862851044618309\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 81.66it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  9%|▊         | 9/105 [00:00<00:01, 83.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3523809523809525 mae: 0.4348789362741759 loss: 3.086724007687051\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 89.31it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7047619047619045 mae: 0.2163521432955038 loss: 0.7725200858359592\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "-------------------------------------------------\n",
      "Test phase\n",
      "Testing X3\n",
      "Running validation on subject ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/103 [00:00<00:22,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 5/103 [00:01<00:23,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/103 [00:01<00:23,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:01<00:00, 51.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.29126213592233 mae: 0.47323162121544543 loss: 3.245724767628487\n",
      "--------------------report----------------------\n",
      "acc1: 0.6249999999999998 acc2: 0.7904761904761899 test_acc: 0.46250000000000024\n",
      "acc1: 0.6374999999999997 acc2: 0.733333333333333 test_acc: 0.6285714285714286\n",
      "acc1: 0.6874999999999996 acc2: 0.7047619047619045 test_acc: 0.29126213592233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train on subject1\n",
    "# validate on subject2, 3\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "report = []\n",
    "\n",
    "for xytst in [(X, y_ohe, 'X1'), (X2, y2_ohe, 'X2'), (X3, y3_ohe, 'X3')]:\n",
    "    \n",
    "    simple_conv = create_model()\n",
    "    simple_conv.compile(loss='categorical_crossentropy', metrics=['acc', 'mae'], optimizer=Adam(lr=0.001))\n",
    "    \n",
    "    xytr = [a for a in [(X, y_ohe), (X2, y2_ohe), (X3, y3_ohe)] if a!=xytst]\n",
    "\n",
    "    EPOCH = 2\n",
    "    train_split = 1.0\n",
    "\n",
    "    simple_conv, acc = train(simple_conv, xytr[0][0], xytr[0][1], EPOCH, train_split)\n",
    "\n",
    "    simple_conv, acc2 = train(simple_conv, xytr[1][0], xytr[1][1], EPOCH, train_split)\n",
    "\n",
    "    print('-------------------------------------------------')\n",
    "    print('Test phase')\n",
    "    print(f'Testing {xytst[2]}')\n",
    "\n",
    "    t_acc = test(simple_conv, xytst[0], xytst[1])\n",
    "    report.append(f'acc1: {acc} acc2: {acc2} test_acc: {t_acc}')\n",
    "\n",
    "\n",
    "print('--------------------report----------------------')\n",
    "for ta in report:\n",
    "    print(ta)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2, micro-activity\n",
    "\n",
    "# CNN-head\n",
    "\n",
    "def create_model_task2():\n",
    "\n",
    "    n_sensors = 4 #5\n",
    "    n_filters = [3, 3, 3, 3] # , 3*29] # X, Y, Z\n",
    "\n",
    "\n",
    "    # make the input layers\n",
    "    ips = []\n",
    "\n",
    "    n_f = 0\n",
    "    for sensor in all_sensors:\n",
    "        ips.append(Input(shape = (None, n_filters[n_f]), name = sensor + '_ipX'))\n",
    "        n_f += 1\n",
    "\n",
    "    print(f'Input tensors: {ips}')\n",
    "\n",
    "    convs = []\n",
    "    for ip in ips:\n",
    "        conv = Conv1D(filters = 16, kernel_size = 1, padding = 'same', activation = None, name = ip.name.split(':')[0] + '_conv')(ip)\n",
    "        convs.append(conv)\n",
    "        \n",
    "    # batch-normalization\n",
    "    bns = []\n",
    "    for conv in convs:\n",
    "        bn = BatchNormalization()(conv)\n",
    "        bns.append(bn)\n",
    "\n",
    "    convs2 = []\n",
    "    for bn in bns:\n",
    "        conv2 = Conv1D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu')(bn)\n",
    "        convs2.append(conv2)\n",
    "        \n",
    "    # batch-normalization\n",
    "    bns2 = []\n",
    "    for conv2 in convs2:\n",
    "        bn2 = BatchNormalization()(conv2)\n",
    "        bns2.append(bn2)\n",
    "\n",
    "\n",
    "\n",
    "    # concatenation\n",
    "\n",
    "    concat = Concatenate(axis=1, name = 'concat')(bns2)\n",
    "\n",
    "    concat_conv = Conv1D(filters = 16, kernel_size = 1, padding = 'same', activation = None, name = 'concat_conv')(concat)\n",
    "    \n",
    "    bn3 = BatchNormalization()(concat_conv)\n",
    "    con_conv2 = Conv1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', name = 'con_conv2')(bn3)\n",
    "    bn4 = BatchNormalization()(con_conv2)\n",
    "    gmp = GlobalMaxPooling1D()(bn4)\n",
    "\n",
    "    # simple FC\n",
    "    final_softmax = Dense(10, activation = 'sigmoid')(gmp)\n",
    "    simple_conv = Model(ips, final_softmax)\n",
    "    return simple_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensors: [<tf.Tensor 'right_arm_ipX_4:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'right_wrist_ipX_4:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_hip_ipX_4:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_wrist_ipX_4:0' shape=(None, None, 3) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "task2_model = create_model_task2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "right_arm_ipX (InputLayer)      [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_wrist_ipX (InputLayer)    [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "left_hip_ipX (InputLayer)       [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "left_wrist_ipX (InputLayer)     [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_arm_ipX_4_conv (Conv1D)   (None, None, 16)     64          right_arm_ipX[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "right_wrist_ipX_4_conv (Conv1D) (None, None, 16)     64          right_wrist_ipX[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "left_hip_ipX_4_conv (Conv1D)    (None, None, 16)     64          left_hip_ipX[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "left_wrist_ipX_4_conv (Conv1D)  (None, None, 16)     64          left_wrist_ipX[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, 16)     64          right_arm_ipX_4_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, 16)     64          right_wrist_ipX_4_conv[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, 16)     64          left_hip_ipX_4_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, 16)     64          left_wrist_ipX_4_conv[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 16)     784         batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 16)     784         batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 16)     784         batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 16)     784         batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, 16)     64          conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, 16)     64          conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, 16)     64          conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, 16)     64          conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, None, 16)     0           batch_normalization_44[0][0]     \n",
      "                                                                 batch_normalization_45[0][0]     \n",
      "                                                                 batch_normalization_46[0][0]     \n",
      "                                                                 batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concat_conv (Conv1D)            (None, None, 16)     272         concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, 16)     64          concat_conv[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "con_conv2 (Conv1D)              (None, None, 32)     1568        batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, 32)     128         con_conv2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 32)           0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           330         global_max_pooling1d_4[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 6,266\n",
      "Trainable params: 5,914\n",
      "Non-trainable params: 352\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "task2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task2(simple_conv, X, y_ohe, EPOCH, train_split=1.0):\n",
    "    # single zero padding for NULL datas\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            if len(X) == 5 and i == len(X)-1: # assuming mocap always comes in the last index\n",
    "                if len(X[i][j]) == 0:\n",
    "                    X[i][j] = np.zeros((1, 87))\n",
    "            else:\n",
    "                if len(X[i][j]) == 0:\n",
    "                    X[i][j] = np.array([[0, 0, 0]])\n",
    "\n",
    "    report_acc = 0.0\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f'Training epoch {epoch} ...')\n",
    "        avg_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        avg_mae = 0.0\n",
    "\n",
    "        for sample_i in tqdm(range(int(len(X[0])*train_split))):\n",
    "            [train_loss, acc, mae]   =  simple_conv.train_on_batch([X[0][sample_i].reshape(1,len(X[0][sample_i]),3), \n",
    "                                        X[1][sample_i].reshape(1,len(X[1][sample_i]),3), \n",
    "                                        X[2][sample_i].reshape(1,len(X[2][sample_i]),3), \n",
    "                                        X[3][sample_i].reshape(1,len(X[3][sample_i]),3)],\n",
    "                                        #X[4][sample_i].reshape(1,len(X[4][sample_i]),3*29),], \n",
    "                                        y_ohe[sample_i].reshape(1,10))\n",
    "            avg_loss += train_loss/(len(X[0])*train_split)\n",
    "            avg_acc += acc/(len(X[0])*train_split)\n",
    "            avg_mae += mae/(len(X[0])*train_split)\n",
    "        report_acc = avg_acc\n",
    "        print(f'acc: {avg_acc} mae: {avg_mae} loss: {avg_loss}')\n",
    "        print('Running validation ...')\n",
    "        avg_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        avg_mae = 0.0\n",
    "        for sample_i in tqdm(range(int(len(X[0])*train_split), len(X[0]))):\n",
    "            [test_loss, acc, mae]   =  simple_conv.test_on_batch([X[0][sample_i].reshape(1,len(X[0][sample_i]),3), \n",
    "                                        X[1][sample_i].reshape(1,len(X[1][sample_i]),3), \n",
    "                                        X[2][sample_i].reshape(1,len(X[2][sample_i]),3), \n",
    "                                        X[3][sample_i].reshape(1,len(X[3][sample_i]),3)],\n",
    "                                        #X[4][sample_i].reshape(1,len(X[4][sample_i]),3*29)], \n",
    "                                        y_ohe[sample_i].reshape(1,10))\n",
    "            avg_loss += test_loss/(len(X[0])*(1-train_split))\n",
    "            avg_acc += acc/(len(X[0])*(1-train_split))\n",
    "            avg_mae += mae/(len(X[0])*(1-train_split))\n",
    "            \n",
    "        print(f'test acc: {avg_acc} mae: {avg_mae} loss: {avg_loss}')\n",
    "        \n",
    "    return simple_conv, report_acc\n",
    "\n",
    "def test_task2(simple_conv, X2, y2_ohe):\n",
    "    for i in range(len(X2)):\n",
    "        for j in range(len(X2[0])):\n",
    "            if len(X2) == 5 and i == len(X2)-1: # assuming mocap always comes in the last index\n",
    "                if len(X2[i][j]) == 0:\n",
    "                    X2[i][j] = np.zeros((1, 87))\n",
    "            else:\n",
    "                if len(X2[i][j]) == 0:\n",
    "                    X2[i][j] = np.array([[0, 0, 0]])\n",
    "\n",
    "    print('Running validation on subject ...')\n",
    "    avg_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    avg_mae = 0.0\n",
    "    for sample_i in tqdm(range(int(len(X2[0])))):\n",
    "        [test_loss, acc, mae]   =  simple_conv.test_on_batch([X2[0][sample_i].reshape(1,len(X2[0][sample_i]),3), \n",
    "                                    X2[1][sample_i].reshape(1,len(X2[1][sample_i]),3), \n",
    "                                    X2[2][sample_i].reshape(1,len(X2[2][sample_i]),3), \n",
    "                                    X2[3][sample_i].reshape(1,len(X2[3][sample_i]),3)],\n",
    "                                    #X2[4][sample_i].reshape(1,len(X2[4][sample_i]),3*29)], \n",
    "                                    y2_ohe[sample_i].reshape(1,10))\n",
    "        avg_loss += test_loss/(len(X2[0]))\n",
    "        avg_acc += acc/(len(X2[0]))\n",
    "        avg_mae += mae/(len(X2[0]))\n",
    "    print(f'test acc: {avg_acc} mae: {avg_mae} loss: {avg_loss}')\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensors: [<tf.Tensor 'right_arm_ipX_5:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'right_wrist_ipX_5:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_hip_ipX_5:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_wrist_ipX_5:0' shape=(None, None, 3) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:04<01:19,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_on_batch at 0x7f33be45b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:09<00:00,  8.43it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 12%|█▎        | 10/80 [00:00<00:00, 92.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.0625 mae: 0.4550256929360329 loss: 16.952603004872792\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 86.16it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  9%|▊         | 9/105 [00:00<00:01, 86.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.0875 mae: 0.4962710810825228 loss: 8.546319580078123\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 91.20it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 11%|█▏        | 12/105 [00:00<00:00, 118.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.15238095238095234 mae: 0.44027344541890284 loss: 7.326225419271561\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 79.71it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.1904761904761904 mae: 0.38262380418323333 loss: 6.820691750163124\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "-------------------------------------------------\n",
      "Test phase\n",
      "Testing X1\n",
      "Running validation on subject ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:00<00:18,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:02<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.2875000000000001 mae: 0.3961328264325858 loss: 6.475295117497444\n",
      "Input tensors: [<tf.Tensor 'right_arm_ipX_6:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'right_wrist_ipX_6:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_hip_ipX_6:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_wrist_ipX_6:0' shape=(None, None, 3) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:03<01:15,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_on_batch at 0x7f33be45b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:09<00:00,  8.82it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 12%|█▎        | 10/80 [00:00<00:00, 92.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.0625 mae: 0.5072760779410602 loss: 9.104968528449529\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:01<00:00, 75.37it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  9%|▊         | 9/105 [00:00<00:01, 88.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.09999999999999999 mae: 0.41381542682647715 loss: 4.305423058569434\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 106.05it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 10%|█         | 11/105 [00:00<00:00, 100.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.24761904761904752 mae: 0.4141566007619813 loss: 4.97007235118321\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 109.99it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.25714285714285706 mae: 0.37024850753091637 loss: 4.343767001515342\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "-------------------------------------------------\n",
      "Test phase\n",
      "Testing X2\n",
      "Running validation on subject ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/105 [00:00<00:20,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/105 [00:01<00:16,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:02<00:00, 47.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.27619047619047615 mae: 0.3903626236887205 loss: 5.415211650303431\n",
      "Input tensors: [<tf.Tensor 'right_arm_ipX_7:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'right_wrist_ipX_7:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_hip_ipX_7:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'left_wrist_ipX_7:0' shape=(None, None, 3) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/80 [00:04<01:24,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_on_batch at 0x7f33be45b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:09<00:00,  8.17it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 12%|█▎        | 10/80 [00:00<00:00, 95.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.025 mae: 0.5973854752257465 loss: 12.380555552244184\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 84.65it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 10%|█         | 11/105 [00:00<00:00, 103.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.05 mae: 0.5569828594103455 loss: 11.44597551822662\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 105.59it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 10%|▉         | 10/105 [00:00<00:01, 92.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.06666666666666668 mae: 0.5693720171848936 loss: 10.68868492217291\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "Training epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:01<00:00, 91.79it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/103 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.11428571428571427 mae: 0.5567531516154604 loss: 10.521253375780013\n",
      "Running validation ...\n",
      "test acc: 0.0 mae: 0.0 loss: 0.0\n",
      "-------------------------------------------------\n",
      "Test phase\n",
      "Testing X3\n",
      "Running validation on subject ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/103 [00:00<00:22,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 5/103 [00:01<00:21,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/103 [00:01<00:22,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function test_on_batch at 0x7f33be45b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:02<00:00, 50.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.009708737864077669 mae: 0.504395227264432 loss: 11.941288818433458\n",
      "--------------------report----------------------\n",
      "acc1: 0.0875 acc2: 0.1904761904761904 test_acc: 0.2875000000000001\n",
      "acc1: 0.09999999999999999 acc2: 0.25714285714285706 test_acc: 0.27619047619047615\n",
      "acc1: 0.05 acc2: 0.11428571428571427 test_acc: 0.009708737864077669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train on subject1\n",
    "# validate on subject2, 3\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "report = []\n",
    "\n",
    "for xytst in [(X, y_ml_ohe, 'X1'), (X2, y_ml2_ohe, 'X2'), (X3, y_ml3_ohe, 'X3')]:\n",
    "    \n",
    "    simple_conv_task2 = create_model_task2()\n",
    "    simple_conv_task2.compile(loss='categorical_crossentropy', metrics=['acc', 'mae'], optimizer=Adam(lr=0.001))\n",
    "    \n",
    "    xytr = [a for a in [(X, y_ml_ohe), (X2, y_ml2_ohe), (X3, y_ml3_ohe)] if a!=xytst]\n",
    "\n",
    "    EPOCH = 2\n",
    "    train_split = 1.0\n",
    "\n",
    "    simple_conv_task2, acc = train_task2(simple_conv_task2, xytr[0][0], xytr[0][1], EPOCH, train_split)\n",
    "\n",
    "    simple_conv_task2, acc2 = train_task2(simple_conv_task2, xytr[1][0], xytr[1][1], EPOCH, train_split)\n",
    "\n",
    "    print('-------------------------------------------------')\n",
    "    print('Test phase')\n",
    "    print(f'Testing {xytst[2]}')\n",
    "\n",
    "    t_acc = test_task2(simple_conv_task2, xytst[0], xytst[1])\n",
    "    report.append(f'acc1: {acc} acc2: {acc2} test_acc: {t_acc}')\n",
    "\n",
    "\n",
    "print('--------------------report----------------------')\n",
    "for ta in report:\n",
    "    print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating labels for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nabil/ml_contests/cooking_activity/cook2020_tutorials/train/train\n",
      "/home/nabil/ml_contests/cooking_activity/cook2020_tutorials\n",
      "/home/nabil/ml_contests/cooking_activity/cook2020_tutorials\n",
      "/home/nabil/ml_contests\n"
     ]
    }
   ],
   "source": [
    "cdir = os.getcwd()\n",
    "print(cdir)\n",
    "os.chdir(cdir + '/../../')\n",
    "print(os.getcwd())\n",
    "cdir = os.getcwd()\n",
    "print(cdir)\n",
    "os.chdir(cdir + '/../../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nabil/ml_contests/cooking_activity/cook2020_tutorials/test/test\n"
     ]
    }
   ],
   "source": [
    "os.chdir(cdir + '/test/test')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['right_arm/*.csv', 'right_wrist/*.csv', 'left_hip/*.csv', 'left_wrist/*.csv']\n"
     ]
    }
   ],
   "source": [
    "# reading test set\n",
    "\n",
    "# there's 1 subject\n",
    "subject4 = {}\n",
    "\n",
    "\n",
    "# we will load every single right arm data, separate based on subject id, do feature extraction, run t-SNE\n",
    "\n",
    "all_sensors = ['right_arm', 'right_wrist', 'left_hip', 'left_wrist'] #, 'mocap']\n",
    "\n",
    "data_folder = [f'{sensor}/*.csv' for sensor in all_sensors]\n",
    "\n",
    "print(data_folder)\n",
    "\n",
    "files = glob.glob(data_folder[0])\n",
    "\n",
    "# processing will be done as numpy array\n",
    "\n",
    "subject4['data'] = []\n",
    "\n",
    " \n",
    "# data format : data -> id, process(timeseries, timestamps) : right_arm as numpy array (missing data will be imputed)\n",
    "\n",
    "\n",
    "# id is generated by concatenating INT(subject+trial)\n",
    "\n",
    "for f in files:\n",
    "    #print(f)\n",
    "    c_sub = {} # current subject\n",
    "    if f.split('/')[1].split('_')[0] == 'subject4':\n",
    "        \n",
    "        c_sub['id'] = int(f.split('/')[1].split('_')[0][-1] + f.split('/')[1].split('_')[2].split('.')[0])\n",
    "        #print(ra['id'])\n",
    "        \n",
    "        for sensor in all_sensors:\n",
    "            #print(sensor)\n",
    "            ra = pd.read_csv(sensor + '/' + f.split('/')[1])\n",
    "            #print(ra)\n",
    "            ra.sort_values(by=['timestamp'], inplace = True)\n",
    "            c_sub[sensor] = {}\n",
    "            if sensor == 'mocap':\n",
    "                for axis in mocap_axis:\n",
    "                    c_sub[sensor][axis] = process(ra[axis], ra['timestamp'])\n",
    "            else:\n",
    "                for axis in ['X', 'Y', 'Z']:\n",
    "                    c_sub[sensor][axis] = process(ra[axis], ra['timestamp'])\n",
    "                \n",
    "        subject4['data'].append(c_sub)\n",
    "    else:\n",
    "        print('either new subject or a bug')\n",
    "        \n",
    " \n",
    "\n",
    "        \n",
    "        \n",
    "# re-formatting dataset for training\n",
    "\n",
    "X_gen = []\n",
    "\n",
    "\n",
    "for sensor in all_sensors:\n",
    "    cs_data = []\n",
    "    for i in range(len(subject4['data'])):\n",
    "        sub_data = []\n",
    "        if sensor == 'mocap':\n",
    "            for sig in mocap_axis:\n",
    "                sub_data.append(np.array(subject4['data'][i][sensor][sig]))\n",
    "        else:\n",
    "            for sig in ['X', 'Y', 'Z']:\n",
    "                sub_data.append(np.array(subject4['data'][i][sensor][sig]))\n",
    "        sub_data = np.array(sub_data)\n",
    "        # print(sub_data.shape)\n",
    "        sub_data = np.swapaxes(sub_data, 0, 1)\n",
    "        cs_data.append(sub_data)\n",
    "    X_gen.append(cs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(simple_conv, simple_conv_task2, X2):\n",
    "    for i in range(len(X2)):\n",
    "        for j in range(len(X2[0])):\n",
    "            if len(X2) == 5 and i == len(X2)-1: # assuming mocap always comes in the last index\n",
    "                if len(X2[i][j]) == 0:\n",
    "                    X2[i][j] = np.zeros((1, 87))\n",
    "            else:\n",
    "                if len(X2[i][j]) == 0:\n",
    "                    X2[i][j] = np.array([[0, 0, 0]])\n",
    "\n",
    "    print('Running inference on subject ...')\n",
    "    y = []\n",
    "    for sample_i in range(int(len(X2[0]))):\n",
    "        y_c   =  simple_conv.predict([X2[0][sample_i].reshape(1,len(X2[0][sample_i]),3), \n",
    "                                    X2[1][sample_i].reshape(1,len(X2[1][sample_i]),3), \n",
    "                                    X2[2][sample_i].reshape(1,len(X2[2][sample_i]),3), \n",
    "                                    X2[3][sample_i].reshape(1,len(X2[3][sample_i]),3)]\n",
    "                                    #X2[4][sample_i].reshape(1,len(X2[4][sample_i]),3*29)], \n",
    "                                    )\n",
    "        y.append(list(label_encoder.classes_)[np.argmax(y_c)])\n",
    "        \n",
    "    y_ml = []\n",
    "    \n",
    "    for sample_i in range(int(len(X2[0]))):\n",
    "        y_c   =  simple_conv_task2.predict([X2[0][sample_i].reshape(1,len(X2[0][sample_i]),3), \n",
    "                                    X2[1][sample_i].reshape(1,len(X2[1][sample_i]),3), \n",
    "                                    X2[2][sample_i].reshape(1,len(X2[2][sample_i]),3), \n",
    "                                    X2[3][sample_i].reshape(1,len(X2[3][sample_i]),3)]\n",
    "                                    #X2[4][sample_i].reshape(1,len(X2[4][sample_i]),3*29)], \n",
    "                               )\n",
    "        idx = 0\n",
    "        label_str = ''\n",
    "        #print(y_c)\n",
    "        #print(y_c[0])\n",
    "        for yc in y_c[0]:\n",
    "            #print(yc)\n",
    "            if yc >= 0.5:\n",
    "                label_str += list(mlb.classes_)[idx] + ','\n",
    "            idx += 1\n",
    "        y_ml.append(label_str)\n",
    "    return y, y_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on subject ...\n"
     ]
    }
   ],
   "source": [
    "y_gen, y_ml_gen = inference(simple_conv, simple_conv_task2, X_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Add', 'Cut', 'Mix', 'Open', 'Peel', 'Pour', 'Put', 'Take', 'Wash',\n",
       "       'other'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cut,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Cut,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,other,',\n",
       " 'Cut,Open,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Pour,Put,other,',\n",
       " 'Add,Open,Wash,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Cut,Open,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Open,Peel,Pour,Put,other,',\n",
       " 'Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Open,Peel,Put,Wash,other,',\n",
       " 'Cut,Open,Put,Wash,other,',\n",
       " 'Add,Cut,Open,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Cut,Mix,Peel,Pour,Put,Wash,other,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Pour,Put,other,',\n",
       " 'Cut,Mix,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Cut,Put,other,',\n",
       " 'Add,Cut,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Cut,Mix,Open,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Open,Wash,',\n",
       " 'Add,Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Pour,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Pour,Put,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Open,Put,Wash,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Cut,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Open,Peel,Pour,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Open,Peel,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Pour,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Put,other,',\n",
       " 'Cut,Open,Wash,',\n",
       " 'Add,Cut,Open,Peel,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,Wash,other,',\n",
       " 'Cut,Open,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Open,Peel,Pour,Put,other,',\n",
       " 'Cut,Open,Peel,Pour,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Cut,Put,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Cut,Mix,Peel,Pour,Put,Wash,other,',\n",
       " 'Cut,Mix,Peel,Put,Wash,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Peel,Put,other,',\n",
       " 'Add,Cut,Mix,Peel,Pour,Put,other,']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ml_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = [f.split('/')[-1].split('.')[0] for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subject4_file_100',\n",
       " 'subject4_file_360',\n",
       " 'subject4_file_313',\n",
       " 'subject4_file_363',\n",
       " 'subject4_file_776',\n",
       " 'subject4_file_262',\n",
       " 'subject4_file_623',\n",
       " 'subject4_file_117',\n",
       " 'subject4_file_112',\n",
       " 'subject4_file_739',\n",
       " 'subject4_file_783',\n",
       " 'subject4_file_777',\n",
       " 'subject4_file_209',\n",
       " 'subject4_file_505',\n",
       " 'subject4_file_335',\n",
       " 'subject4_file_539',\n",
       " 'subject4_file_327',\n",
       " 'subject4_file_63',\n",
       " 'subject4_file_115',\n",
       " 'subject4_file_470',\n",
       " 'subject4_file_366',\n",
       " 'subject4_file_235',\n",
       " 'subject4_file_918',\n",
       " 'subject4_file_237',\n",
       " 'subject4_file_38',\n",
       " 'subject4_file_538',\n",
       " 'subject4_file_186',\n",
       " 'subject4_file_61',\n",
       " 'subject4_file_225',\n",
       " 'subject4_file_325',\n",
       " 'subject4_file_174',\n",
       " 'subject4_file_256',\n",
       " 'subject4_file_553',\n",
       " 'subject4_file_165',\n",
       " 'subject4_file_984',\n",
       " 'subject4_file_41',\n",
       " 'subject4_file_633',\n",
       " 'subject4_file_521',\n",
       " 'subject4_file_267',\n",
       " 'subject4_file_164',\n",
       " 'subject4_file_843',\n",
       " 'subject4_file_652',\n",
       " 'subject4_file_693',\n",
       " 'subject4_file_746',\n",
       " 'subject4_file_829',\n",
       " 'subject4_file_419',\n",
       " 'subject4_file_258',\n",
       " 'subject4_file_324',\n",
       " 'subject4_file_993',\n",
       " 'subject4_file_736',\n",
       " 'subject4_file_62',\n",
       " 'subject4_file_276',\n",
       " 'subject4_file_834',\n",
       " 'subject4_file_398',\n",
       " 'subject4_file_68',\n",
       " 'subject4_file_182',\n",
       " 'subject4_file_529',\n",
       " 'subject4_file_774',\n",
       " 'subject4_file_869',\n",
       " 'subject4_file_231',\n",
       " 'subject4_file_962',\n",
       " 'subject4_file_761',\n",
       " 'subject4_file_58',\n",
       " 'subject4_file_106',\n",
       " 'subject4_file_821',\n",
       " 'subject4_file_284',\n",
       " 'subject4_file_856',\n",
       " 'subject4_file_223',\n",
       " 'subject4_file_936',\n",
       " 'subject4_file_23',\n",
       " 'subject4_file_198',\n",
       " 'subject4_file_966',\n",
       " 'subject4_file_995',\n",
       " 'subject4_file_12',\n",
       " 'subject4_file_942',\n",
       " 'subject4_file_495',\n",
       " 'subject4_file_336',\n",
       " 'subject4_file_104',\n",
       " 'subject4_file_960',\n",
       " 'subject4_file_854',\n",
       " 'subject4_file_806',\n",
       " 'subject4_file_817',\n",
       " 'subject4_file_451',\n",
       " 'subject4_file_154',\n",
       " 'subject4_file_968',\n",
       " 'subject4_file_173',\n",
       " 'subject4_file_219',\n",
       " 'subject4_file_286',\n",
       " 'subject4_file_288',\n",
       " 'subject4_file_321',\n",
       " 'subject4_file_7',\n",
       " 'subject4_file_852',\n",
       " 'subject4_file_387',\n",
       " 'subject4_file_161',\n",
       " 'subject4_file_402',\n",
       " 'subject4_file_226',\n",
       " 'subject4_file_961',\n",
       " 'subject4_file_602',\n",
       " 'subject4_file_190',\n",
       " 'subject4_file_986',\n",
       " 'subject4_file_632',\n",
       " 'subject4_file_921',\n",
       " 'subject4_file_675',\n",
       " 'subject4_file_129',\n",
       " 'subject4_file_457',\n",
       " 'subject4_file_121',\n",
       " 'subject4_file_414',\n",
       " 'subject4_file_838',\n",
       " 'subject4_file_646',\n",
       " 'subject4_file_634',\n",
       " 'subject4_file_975',\n",
       " 'subject4_file_18',\n",
       " 'subject4_file_253',\n",
       " 'subject4_file_985',\n",
       " 'subject4_file_1',\n",
       " 'subject4_file_613',\n",
       " 'subject4_file_592',\n",
       " 'subject4_file_706',\n",
       " 'subject4_file_651',\n",
       " 'subject4_file_650',\n",
       " 'subject4_file_578',\n",
       " 'subject4_file_97',\n",
       " 'subject4_file_274',\n",
       " 'subject4_file_118',\n",
       " 'subject4_file_544',\n",
       " 'subject4_file_278',\n",
       " 'subject4_file_456',\n",
       " 'subject4_file_458',\n",
       " 'subject4_file_3',\n",
       " 'subject4_file_779',\n",
       " 'subject4_file_418',\n",
       " 'subject4_file_822',\n",
       " 'subject4_file_166',\n",
       " 'subject4_file_835',\n",
       " 'subject4_file_571',\n",
       " 'subject4_file_842',\n",
       " 'subject4_file_171',\n",
       " 'subject4_file_392',\n",
       " 'subject4_file_630',\n",
       " 'subject4_file_860',\n",
       " 'subject4_file_537',\n",
       " 'subject4_file_826',\n",
       " 'subject4_file_317',\n",
       " 'subject4_file_624',\n",
       " 'subject4_file_969',\n",
       " 'subject4_file_737',\n",
       " 'subject4_file_17',\n",
       " 'subject4_file_668',\n",
       " 'subject4_file_180',\n",
       " 'subject4_file_704',\n",
       " 'subject4_file_957',\n",
       " 'subject4_file_6',\n",
       " 'subject4_file_282',\n",
       " 'subject4_file_443',\n",
       " 'subject4_file_711',\n",
       " 'subject4_file_396',\n",
       " 'subject4_file_727',\n",
       " 'subject4_file_358',\n",
       " 'subject4_file_439',\n",
       " 'subject4_file_980',\n",
       " 'subject4_file_156',\n",
       " 'subject4_file_978',\n",
       " 'subject4_file_714',\n",
       " 'subject4_file_183',\n",
       " 'subject4_file_563',\n",
       " 'subject4_file_302',\n",
       " 'subject4_file_307',\n",
       " 'subject4_file_22',\n",
       " 'subject4_file_877',\n",
       " 'subject4_file_676',\n",
       " 'subject4_file_947',\n",
       " 'subject4_file_295',\n",
       " 'subject4_file_285',\n",
       " 'subject4_file_176',\n",
       " 'subject4_file_125',\n",
       " 'subject4_file_26',\n",
       " 'subject4_file_871',\n",
       " 'subject4_file_812',\n",
       " 'subject4_file_718',\n",
       " 'subject4_file_369']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {'file_id': file_ids, 'macro': y_gen, 'micro': y_ml_gen}\n",
    "out_csv = pd.DataFrame(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv.to_csv('cc_PseudoEmpirical_submission1.csv', index = False, header = False, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
